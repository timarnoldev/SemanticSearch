{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Semantische Suche\n",
        "\n",
        "Erstellt Embeddings (Vektoren), die die Bedeutung der gegebenen Inhalte in 200 Dimensionen kodiert. Für die Umwandlung von Text bzw. Wörtern in sog. Embeddings wurde ein Model trainiert, auf das hier zurückgegriffen wird.\n",
        "\n",
        "Folgendes Beispiel zeigt, wie man mit diesen Vektoren rechnen kann. Subtrahiert man woman vom Wort mother und addiert man man erhält man father, wie du im folgenden Beispiel siehst. (Hinweis: Der Download kann etwas länger dauern).\n",
        "\n",
        "Hast du noch weitere Ideen, mit welchen Wörtern man rechnen kann?\n",
        "\n",
        "z.B. teacher - school + university = professor\n",
        "z.B. pretzel - germany + france ≈ baguette\n"
      ],
      "metadata": {
        "id": "OYtF1Gem5-0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXaNXzn_lxVi"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"glove-wiki-gigaword-200\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = model[\"mother\"] - model[\"woman\"] + model[\"man\"]\n",
        "\n",
        "\n",
        "model.similar_by_vector(vec, topn=5)"
      ],
      "metadata": {
        "id": "in3MN8bB9o7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im nächsten Beispiel nutzen wir diesen Technologie, um die Bedeutung von Texten zu durchsuchen. Dafür nutzen wir eine Liste an Texten und ein Suchbegriff, die von einem Modell in Vektoren umgewandelt werden. Mithilfe des Skalarprodukts kann dann über alle Dimensionen die Ähnlichkeit von zwei Vektoren ermittelt werden. Dabei ist der Wert größer je \"mehr\" die Vektoren in ihrer Richtung übereinstimmen. Dabei ist es einfacher sich das ganze im 3-Dimensionalen Raum vorzustellen.\n",
        "\n",
        "Für diese Aufgabe ist allerdings ein anderes Model nötig, da dieses ganze Sätze verstehen kann. Dies konvertiert die Sätze in 768 dimensionale Vektoren."
      ],
      "metadata": {
        "id": "RXvde_6RTkKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-mpnet-base-v2\")"
      ],
      "metadata": {
        "id": "7Gt0B52CVkfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [\n",
        "    \"Deep learning models are revolutionizing natural language processing.\",\n",
        "    \"Reinforcement learning algorithms are used in robotics and game playing.\",\n",
        "    \"Generative adversarial networks are creating realistic images and videos.\",\n",
        "    \"Explainable AI is crucial for building trust in AI systems.\",\n",
        "    \"Ethical considerations in AI development are becoming increasingly important.\"\n",
        "]\n",
        "\n",
        "len(sentences)"
      ],
      "metadata": {
        "id": "Q5JgKETWVsTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodings = model.encode(sentences)\n",
        "\n",
        "# Das ist die Frage, die mit den gebenen Sätze verglichen wird?\n",
        "# Welcher Satz passt den deiner Meinung nach zu folgender Frage?\n",
        "query = \"Which model is used for media creation?\"\n",
        "\n",
        "queryEncoding = model.encode(query)\n",
        "\n",
        "# Mithilfe der Softmax Funktion werden die Ähnlichkeiten normiert und als Wahrscheinlichkeitsverteilung ausgegeben\n",
        "result = torch.nn.Softmax(1)(util.cos_sim(queryEncoding, encodings),)\n",
        "result\n"
      ],
      "metadata": {
        "id": "A9-tKllOWRWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Der höchste Wert wird gesucht und der beste passendste Satz wird ausgegeben\n",
        "\n",
        "# Die Argmax-Funktion gibt den Index des höchsten Werts zurück\n",
        "highest = torch.argmax(result)\n",
        "print(highest)\n",
        "print(sentences[highest])\n",
        "#"
      ],
      "metadata": {
        "id": "RyjOeXnjaP47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Aufgabe: Finde drei Wortpaare, die sich semantisch ähnlich sind (z.B. glücklich - zufrieden).\n",
        "Schwierigkeitsgrad: leicht\n",
        "\n",
        "- Aufgabe: Erkläre mit eigenen Worten, was ein Embedding ist.\n",
        "Schwierigkeitsgrad: leicht\n",
        "\n",
        "- Aufgabe: Verändere den Suchbegriff (query) im Code und beobachte, wie sich das Ergebnis ändert. Beschreibe deine Beobachtungen.\n",
        "Schwierigkeitsgrad: mittel\n",
        "\n",
        "- Aufgabe: Füge drei weitere Sätze zur Liste 'sentences' hinzu und überprüfe, wie das Ergebnis der semantischen Suche beeinflusst wird.\n",
        "Schwierigkeitsgrad: mittel\n",
        "\n",
        "- Aufgabe: Recherchiere, welche anderen SentenceTransformer-Modelle es gibt und vergleiche deren Leistung mit dem gegebenen Modell.\n",
        "Schwierigkeitsgrad: mittel\n",
        "\n",
        "- Aufgabe: Implementiere eine eigene Funktion, die die Ähnlichkeit zwischen zwei Sätzen berechnet, ohne die 'util.cos_sim' Funktion zu verwenden.\n",
        "Schwierigkeitsgrad: schwer\n",
        "\n",
        "- Aufgabe: Erweitere den Code so, dass die Top 3 ähnlichsten Sätze ausgegeben werden, statt nur des ähnlichsten.\n",
        "Schwierigkeitsgrad: mittel"
      ],
      "metadata": {
        "id": "O_w8H0t5gOGq"
      }
    }
  ]
}